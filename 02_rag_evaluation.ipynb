{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nest_asyncio\n",
    "import qdrant_client\n",
    "\n",
    "from llama_index.core import Settings\n",
    "from llama_index.core import PromptTemplate\n",
    "from llama_index.llms.ollama import Ollama\n",
    "from llama_index.core import StorageContext\n",
    "from llama_index.core import VectorStoreIndex, SimpleDirectoryReader\n",
    "from llama_index.embeddings.fastembed import FastEmbedEmbedding\n",
    "from llama_index.core.postprocessor import SentenceTransformerRerank\n",
    "from llama_index.vector_stores.qdrant import QdrantVectorStore\n",
    "\n",
    "from IPython.display import Markdown, display\n",
    "from dotenv import load_dotenv\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup Asyncio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nest_asyncio\n",
    "\n",
    "nest_asyncio.apply()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define LLM, Embedding model and re-ranker model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "llm = Ollama(model=\"llama3.2:1b\", request_timeout=120.0)\n",
    "\n",
    "embed_model = FastEmbedEmbedding(model_name=\"BAAI/bge-large-en-v1.5\")\n",
    "\n",
    "rerank = SentenceTransformerRerank(model=\"BAAI/bge-reranker-base\", top_n=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "Settings.embed_model = embed_model\n",
    "Settings.llm = llm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Read the documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_dir_path = './docs/paul_graham'\n",
    "\n",
    "loader = SimpleDirectoryReader(\n",
    "            input_dir = input_dir_path,\n",
    "            required_exts=[\".txt\"],\n",
    "            recursive=True\n",
    "        )\n",
    "docs = loader.load_data()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set up the Qdrant vector database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "client = qdrant_client.QdrantClient(host=\"localhost\", port=6333)\n",
    "vector_store = QdrantVectorStore(client=client,\n",
    "                                 collection_name=\"document_chat\")\n",
    "storage_context = StorageContext.from_defaults(vector_store=vector_store)\n",
    "index = VectorStoreIndex.from_documents(docs,\n",
    "                                        storage_context=storage_context)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define the query engine and prompt template"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "query_engine = index.as_query_engine(similarity_top_k=4,\n",
    "                                     node_postprocessors=[rerank])\n",
    "\n",
    "template = \"\"\"Context information is below.\n",
    "              ---------------------\n",
    "              {context_str}\n",
    "              ---------------------\n",
    "              Given the context information above I want you to think\n",
    "              step by step to answer the query in a crisp manner. Incase \n",
    "              you don't know the answer say 'I don't know!'.\n",
    "              \n",
    "              Query: {query_str}\n",
    "              \n",
    "              Answer:\"\"\"\n",
    "\n",
    "qa_prompt_tmpl = PromptTemplate(template)\n",
    "\n",
    "query_engine.update_prompts(\n",
    "    {\"response_synthesizer:text_qa_template\": qa_prompt_tmpl}\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Query the document"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "response = query_engine.query(\"\"\"How did the structure of funding startups \n",
    "                                 in batches contribute to the success and \n",
    "                                 growth of the Y Combinator program and the\n",
    "                                 startups involved?\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display(Markdown(str(response)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate the dataset for evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load the knowledge base"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.document_loaders import DirectoryLoader\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "\n",
    "loader = DirectoryLoader(\"./docs/paul_graham/\")\n",
    "\n",
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size=1024, chunk_overlap=20)\n",
    "\n",
    "documents = loader.load_and_split(text_splitter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "documents[0].to_json()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup the models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.llms import Ollama\n",
    "from langchain_community.embeddings import OllamaEmbeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "generator_llm = Ollama(model=\"phi3:3.8b\")\n",
    "critic_llm = Ollama(model=\"llama3.2:1b\")\n",
    "\n",
    "ollama_emb = OllamaEmbeddings(\n",
    "    model=\"nomic-embed-text\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create Ragas Testset Generator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ragas.testset.generator import TestsetGenerator\n",
    "from ragas.testset.evolutions import simple, reasoning, multi_context\n",
    "\n",
    "generator = TestsetGenerator.from_langchain(\n",
    "    generator_llm=generator_llm,\n",
    "    critic_llm=critic_llm,\n",
    "    embeddings=ollama_emb\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Import the csv file\n",
    "import pandas as pd\n",
    "test_df = pd.read_csv(\"./docs/test_data_paul_graham.csv\")\n",
    "test_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluate the RAG pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_response(query_engine, question):\n",
    "    response = query_engine.query(question)\n",
    "    return {\n",
    "        \"answer\": response.response,\n",
    "        \"contexts\": [c.node.get_content() for c in response.source_nodes],\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "distribution = {simple: 0.5, reasoning: 0.25, multi_context: 0.25}\n",
    "\n",
    "total_distribution = sum(distribution.values())\n",
    "\n",
    "if total_distribution != 1:\n",
    "    raise ValueError(\"The sum of the distribution values must equal 1.\")\n",
    "\n",
    "try:\n",
    "    testset = generator.generate_with_langchain_docs(documents,\n",
    "                                                 test_size=min(10, len(documents)), # Adjust test_size to not exceed available documents\n",
    "                                                 distributions=distribution,\n",
    "                                                 raise_exceptions=False,\n",
    "                                                 )\n",
    "except Exception as e:\n",
    "    print(f\"An error occurred: {e}\")\n",
    "    print(f\"Debugging logs: {e.debug_logs}\")\n",
    "\n",
    "test_df = testset.to_pandas().dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import Dataset\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "test_questions = test_df[\"question\"].values\n",
    "\n",
    "responses = [generate_response(query_engine, q) for q in tqdm(test_questions)]\n",
    "\n",
    "dataset_dict = {\n",
    "    \"question\": test_questions,\n",
    "    \"answer\": [response[\"answer\"] for response in responses],\n",
    "    \"contexts\": [response[\"contexts\"] for response in responses],\n",
    "    \"ground_truth\": test_df[\"ground_truth\"].values.tolist(),\n",
    "}\n",
    "\n",
    "ragas_eval_dataset = Dataset.from_dict(dataset_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.embeddings import OllamaEmbeddings\n",
    "from langchain_community.llms import Ollama\n",
    "\n",
    "from ragas import evaluate\n",
    "from ragas.metrics import (\n",
    "    faithfulness,\n",
    "    answer_correctness,\n",
    "    context_recall,\n",
    "    context_precision,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics = [faithfulness, answer_correctness,\n",
    "           context_recall, context_precision]\n",
    "\n",
    "critic_llm = Ollama(model=\"llama3.2:1b\")\n",
    "\n",
    "ollama_emb = OllamaEmbeddings(model=\"nomic-embed-text\")\n",
    "\n",
    "evaluation_result = evaluate(\n",
    "    llm=critic_llm,\n",
    "    embeddings=ollama_emb,\n",
    "    dataset=ragas_eval_dataset,\n",
    "    metrics=metrics\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_scores_df = pd.DataFrame(evaluation_result.scores)\n",
    "eval_scores_df.to_csv(\"./docs/evaluation_scores_paul_graham.csv\", index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ragEnv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
