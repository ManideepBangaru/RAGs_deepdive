{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/manideepbangaru/Documents/Learnings/RAGs_deepdive/ragEnv/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "# Step 1: Load the SQuAD dataset\n",
    "dataset = load_dataset(\"squad\")\n",
    "\n",
    "# Step 2: Extract unique contexts from the dataset\n",
    "data = [item[\"context\"] for item in dataset[\"train\"]]\n",
    "texts = list(set(data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'In Prussia, some officials considered a war against France both inevitable and necessary to arouse German nationalism in those states that would allow the unification of a great German empire. This aim was epitomized by Prussian Chancellor Otto von Bismarck\\'s later statement: \"I did not doubt that a Franco-German war must take place before the construction of a United Germany could be realised.\" Bismarck also knew that France should be the aggressor in the conflict to bring the southern German states to side with Prussia, hence giving Germans numerical superiority. Many Germans also viewed the French as the traditional destabilizer of Europe, and sought to weaken France to prevent further breaches of the peace.'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "texts[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Embed dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.embeddings.huggingface import HuggingFaceEmbedding\n",
    "\n",
    "def batch_iterate(lst, batch_size):\n",
    "    for i in range(0, len(lst), batch_size):\n",
    "        yield lst[i : i + batch_size]\n",
    "\n",
    "class EmbedData:\n",
    "    def __init__(self, \n",
    "                 embed_model_name=\"nomic-ai/nomic-embed-text-v1.5\",\n",
    "                 batch_size=32):\n",
    "        \n",
    "        self.embed_model_name = embed_model_name\n",
    "        self.embed_model = self._load_embed_model()\n",
    "        self.batch_size = batch_size\n",
    "        self.embeddings = []\n",
    "        \n",
    "    def _load_embed_model(self):\n",
    "        embed_model = HuggingFaceEmbedding(model_name=self.embed_model_name,\n",
    "                                           trust_remote_code=True,\n",
    "                                           cache_folder='./hf_cache')\n",
    "        return embed_model\n",
    "    \n",
    "    def generate_embedding(self, context):\n",
    "        return self.embed_model.get_text_embedding_batch(context)\n",
    "    \n",
    "    def embed(self, contexts):\n",
    "        self.contexts = contexts\n",
    "        \n",
    "        for batch_context in tqdm(batch_iterate(contexts, self.batch_size),\n",
    "                                  total=len(contexts)//self.batch_size,\n",
    "                                  desc=\"Embedding data in batches\"):\n",
    "                                  \n",
    "            batch_embeddings = self.generate_embedding(batch_context)\n",
    "            \n",
    "            self.embeddings.extend(batch_embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<All keys matched successfully>\n"
     ]
    }
   ],
   "source": [
    "from tqdm.auto import tqdm\n",
    "\n",
    "batch_size = 32\n",
    "\n",
    "embeddata = EmbedData(batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Embedding data in batches: 4it [00:37,  9.43s/it]                       \n"
     ]
    }
   ],
   "source": [
    "embeddata.embed(texts[:100]) # Embed the first two contexts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import pickle\n",
    "\n",
    "# with open(\"embeddata_full.pickle\", \"rb\") as h:\n",
    "#     embeddata = pickle.load(h)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'In Prussia, some officials considered a war against France both inevitable and necessary to arouse German nationalism in those states that would allow the unification of a great German empire. This aim was epitomized by Prussian Chancellor Otto von Bismarck\\'s later statement: \"I did not doubt that a Franco-German war must take place before the construction of a United Germany could be realised.\" Bismarck also knew that France should be the aggressor in the conflict to bring the southern German states to side with Prussia, hence giving Germans numerical superiority. Many Germans also viewed the French as the traditional destabilizer of Europe, and sought to weaken France to prevent further breaches of the peace.'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embeddata.contexts[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Vector database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from qdrant_client import QdrantClient, models\n",
    "\n",
    "class QdrantVDB:\n",
    "\n",
    "    def __init__(self, collection_name, vector_dim=768, batch_size=512):\n",
    "        self.collection_name = collection_name\n",
    "        self.batch_size = batch_size\n",
    "        self.vector_dim = vector_dim\n",
    "    \n",
    "    def define_client(self):\n",
    "        self.client = QdrantClient(url=\"http://localhost:6333\",\n",
    "                                   prefer_grpc=True)\n",
    "    \n",
    "    def create_collection(self):\n",
    "        \n",
    "        if not self.client.collection_exists(collection_name=self.collection_name):\n",
    "\n",
    "            self.client.create_collection(collection_name=self.collection_name,\n",
    "                                          \n",
    "                                          vectors_config=models.VectorParams(\n",
    "                                                              size=self.vector_dim,\n",
    "                                                              distance=models.Distance.DOT,\n",
    "                                                              on_disk=True),\n",
    "                                          \n",
    "                                          optimizers_config=models.OptimizersConfigDiff(\n",
    "                                                                            default_segment_number=5,\n",
    "                                                                            indexing_threshold=0)\n",
    "                                         )\n",
    "    \n",
    "    def ingest_data(self, embeddata):\n",
    "    \n",
    "        for batch_context, batch_embeddings in tqdm(zip(batch_iterate(embeddata.contexts, self.batch_size), \n",
    "                                                        batch_iterate(embeddata.embeddings, self.batch_size)), \n",
    "                                                    total=len(embeddata.contexts)//self.batch_size, \n",
    "                                                    desc=\"Ingesting in batches\"):\n",
    "        \n",
    "            self.client.upload_collection(collection_name=self.collection_name,\n",
    "                                        vectors=batch_embeddings,\n",
    "                                        payload=[{\"context\": context} for context in batch_context])\n",
    "\n",
    "        self.client.update_collection(collection_name=self.collection_name,\n",
    "                                    optimizer_config=models.OptimizersConfigDiff(indexing_threshold=20000)\n",
    "                                    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Ingesting in batches: 1it [00:00, 11.82it/s]\n"
     ]
    }
   ],
   "source": [
    "database = QdrantVDB(\"squad_collection\")\n",
    "database.define_client()\n",
    "database.create_collection()\n",
    "database.ingest_data(embeddata)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Retrievar class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "class Retriever:\n",
    "\n",
    "    def __init__(self, vector_db, embeddata):\n",
    "        \n",
    "        self.vector_db = vector_db\n",
    "        self.embeddata = embeddata\n",
    "    \n",
    "    def search(self, query):\n",
    "        query_embedding = self.embeddata.embed_model.get_query_embedding(query)\n",
    "            \n",
    "        # Start the timer\n",
    "        start_time = time.time()\n",
    "        \n",
    "        result = self.vector_db.client.search(\n",
    "            collection_name=self.vector_db.collection_name,\n",
    "            \n",
    "            query_vector=query_embedding,\n",
    "            \n",
    "            search_params=models.SearchParams(\n",
    "                quantization=models.QuantizationSearchParams(\n",
    "                    ignore=True,\n",
    "                    rescore=True,\n",
    "                    oversampling=2.0,\n",
    "                )\n",
    "            ),\n",
    "            \n",
    "            timeout=1000,\n",
    "        )\n",
    "        \n",
    "        # End the timer\n",
    "        end_time = time.time()\n",
    "        elapsed_time = end_time - start_time\n",
    "\n",
    "        print(f\"Execution time for the search: {elapsed_time:.4f} seconds\")\n",
    "\n",
    "        return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Execution time for the search: 0.0224 seconds\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "ScoredPoint(id='07659848-6f30-477b-b828-3bef46f203dd', version=3, score=0.4932173490524292, payload={'context': 'Cladistics is a technique borrowed from biology, where it was originally named phylogenetic systematics by Willi Hennig. In biology, the technique is used to determine the evolutionary relationships between different species. In its application in textual criticism, the text of a number of different manuscripts is entered into a computer, which records all the differences between them. The manuscripts are then grouped according to their shared characteristics. The difference between cladistics and more traditional forms of statistical analysis is that, rather than simply arranging the manuscripts into rough groupings according to their overall similarity, cladistics assumes that they are part of a branching family tree and uses that assumption to derive relationships between them. This makes it more like an automated approach to stemmatics. However, where there is a difference, the computer does not attempt to decide which reading is closer to the original text, and so does not indicate which branch of the tree is the \"root\"—which manuscript tradition is closest to the original. Other types of evidence must be used for that purpose.'}, vector=None, shard_key=None, order_value=None)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Retriever(database, embeddata).search(\"Sample query\")[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RAG class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.llms.ollama import Ollama\n",
    "\n",
    "class RAG:\n",
    "\n",
    "    def __init__(self,\n",
    "                 retriever,\n",
    "                 llm_name=\"llama3.2:1b\"):\n",
    "        \n",
    "        self.llm_name = llm_name\n",
    "        self.llm = self._setup_llm()\n",
    "        self.retriever = retriever\n",
    "        self.qa_prompt_tmpl_str = \"\"\"Context information is below.\n",
    "                                     ---------------------\n",
    "                                     {context}\n",
    "                                     ---------------------\n",
    "                                     \n",
    "                                     Given the context information above I want you\n",
    "                                     to think step by step to answer the query in a\n",
    "                                     crisp manner, incase case you don't know the\n",
    "                                     answer say 'I don't know!'\n",
    "                                     \n",
    "                                     ---------------------\n",
    "                                     Query: {query}\n",
    "                                     ---------------------\n",
    "                                     Answer: \"\"\"\n",
    "    \n",
    "    def _setup_llm(self):\n",
    "        return Ollama(model=self.llm_name)\n",
    "    \n",
    "    def generate_context(self, query):\n",
    "    \n",
    "        result = self.retriever.search(query)\n",
    "        context = [dict(data) for data in result]\n",
    "        combined_prompt = []\n",
    "\n",
    "        for entry in context:\n",
    "            context = entry[\"payload\"][\"context\"]\n",
    "\n",
    "            combined_prompt.append(context)\n",
    "\n",
    "        return \"\\n\\n---\\n\\n\".join(combined_prompt)\n",
    "    \n",
    "    def query(self, query):\n",
    "        context = self.generate_context(query=query)\n",
    "        print(\"context is extracted successfully !!!\")\n",
    "        prompt = self.qa_prompt_tmpl_str.format(context=context,\n",
    "                                                query=query)\n",
    "        response = self.llm.complete(prompt)\n",
    "        \n",
    "        return dict(response)['text']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Execution time for the search: 0.0043 seconds\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "ScoredPoint(id='07659848-6f30-477b-b828-3bef46f203dd', version=3, score=0.4932173490524292, payload={'context': 'Cladistics is a technique borrowed from biology, where it was originally named phylogenetic systematics by Willi Hennig. In biology, the technique is used to determine the evolutionary relationships between different species. In its application in textual criticism, the text of a number of different manuscripts is entered into a computer, which records all the differences between them. The manuscripts are then grouped according to their shared characteristics. The difference between cladistics and more traditional forms of statistical analysis is that, rather than simply arranging the manuscripts into rough groupings according to their overall similarity, cladistics assumes that they are part of a branching family tree and uses that assumption to derive relationships between them. This makes it more like an automated approach to stemmatics. However, where there is a difference, the computer does not attempt to decide which reading is closer to the original text, and so does not indicate which branch of the tree is the \"root\"—which manuscript tradition is closest to the original. Other types of evidence must be used for that purpose.'}, vector=None, shard_key=None, order_value=None)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Retriever(database, embeddata).search(\"Sample query\")[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lets use it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "retriever = Retriever(database, embeddata)\n",
    "\n",
    "rag = RAG(retriever)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'In Latin, papyri from Herculaneum dating before 79 AD (when it was destroyed) have been found that have been written in old Roman cursive, where the early forms of minuscule letters \"d\", \"h\" and \"r\", for example, can already be recognised. According to papyrologist Knut Kleve, \"The theory, then, that the lower-case letters have been developed from the fifth century uncials and the ninth century Carolingian minuscules seems to be wrong.\" Both majuscule and minuscule letters existed, but the difference between the two variants was initially stylistic rather than orthographic and the writing system was still basically unicameral: a given handwritten document could use either one style or the other but these were not mixed. European languages, except for Ancient Greek and Latin, did not make the case distinction before about 1300.[citation needed]'"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embeddata.contexts[16]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Execution time for the search: 0.0100 seconds\n"
     ]
    }
   ],
   "source": [
    "query = \"\"\"What is meant by Politecnico\"\"\"\n",
    "query = \"\"\"what is the difference between majuscule and minuscule \"\"\"\n",
    "\n",
    "answer = rag.query(query)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## display the output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "Based on the provided context information, there doesn't seem to be a clear distinction or explanation for the difference between majuscule (uppercase) and minuscule (lowercase) letters in Latin writing systems. \n",
       "\n",
       "It appears that both majuscule and minuscule letters existed and were used, but with initial stylistic differences rather than being strictly orthographic or having any significant functional differences."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from IPython.display import Markdown, display\n",
    "\n",
    "display(Markdown(str(answer)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'In Latin, papyri from Herculaneum dating before 79 AD (when it was destroyed) have been found that have been written in old Roman cursive, where the early forms of minuscule letters \"d\", \"h\" and \"r\", for example, can already be recognised. According to papyrologist Knut Kleve, \"The theory, then, that the lower-case letters have been developed from the fifth century uncials and the ninth century Carolingian minuscules seems to be wrong.\" Both majuscule and minuscule letters existed, but the difference between the two variants was initially stylistic rather than orthographic and the writing system was still basically unicameral: a given handwritten document could use either one style or the other but these were not mixed. European languages, except for Ancient Greek and Latin, did not make the case distinction before about 1300.[citation needed]'"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embeddata.contexts[16]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Binary Quantization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "from qdrant_client import models\n",
    "from qdrant_client import QdrantClient\n",
    "\n",
    "class QdrantVDB_BQ:\n",
    "\n",
    "    def __init__(self, collection_name, vector_dim=768, batch_size=512):\n",
    "        self.collection_name = collection_name\n",
    "        self.batch_size = batch_size\n",
    "        self.vector_dim = vector_dim\n",
    "    \n",
    "    def define_client(self):\n",
    "        self.client = QdrantClient(url=\"http://localhost:6333\",\n",
    "                                   prefer_grpc=True)\n",
    "        \n",
    "    def create_collection(self):\n",
    "        \n",
    "        if not self.client.collection_exists(collection_name=self.collection_name):\n",
    "\n",
    "            self.client.create_collection(collection_name=self.collection_name,\n",
    "                                          \n",
    "                                          vectors_config=models.VectorParams(\n",
    "                                                              size=self.vector_dim,\n",
    "                                                              distance=models.Distance.DOT,\n",
    "                                                              on_disk=True),\n",
    "                                          \n",
    "                                          optimizers_config=models.OptimizersConfigDiff(\n",
    "                                                                            default_segment_number=5,\n",
    "                                                                            indexing_threshold=0),\n",
    "                                          \n",
    "                                          quantization_config=models.BinaryQuantization(\n",
    "                                                        binary=models.BinaryQuantizationConfig(always_ram=True)),\n",
    "                                         )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "class Retriever:\n",
    "\n",
    "    def __init__(self, vector_db, embeddata):\n",
    "        \n",
    "        self.vector_db = vector_db\n",
    "        self.embeddata = embeddata\n",
    "    \n",
    "    def search(self, query):\n",
    "        query_embedding = self.embeddata.embed_model.get_query_embedding(query)\n",
    "            \n",
    "        # Start the timer\n",
    "        start_time = time.time()\n",
    "        \n",
    "        result = self.vector_db.client.search(\n",
    "            collection_name=self.vector_db.collection_name,\n",
    "            \n",
    "            query_vector=query_embedding,\n",
    "            \n",
    "            search_params=models.SearchParams(\n",
    "                quantization=models.QuantizationSearchParams(\n",
    "                    ignore=False,\n",
    "                    rescore=True,\n",
    "                    oversampling=2.0,\n",
    "                )\n",
    "            ),\n",
    "            \n",
    "            timeout=1000,\n",
    "        )\n",
    "        \n",
    "        # End the timer\n",
    "        end_time = time.time()\n",
    "        elapsed_time = end_time - start_time\n",
    "\n",
    "        print(f\"Execution time for the search: {elapsed_time:.4f} seconds\")\n",
    "\n",
    "        return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Ingesting in batches: 1it [00:00,  6.54it/s]\n"
     ]
    }
   ],
   "source": [
    "database = QdrantVDB(\"squad_collection_qb\")\n",
    "database.define_client()\n",
    "database.create_collection()\n",
    "database.ingest_data(embeddata)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "retriever = Retriever(database, embeddata)\n",
    "\n",
    "rag = RAG(retriever)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Execution time for the search: 0.0077 seconds\n"
     ]
    }
   ],
   "source": [
    "query = \"\"\"what is the difference between majuscule and minuscule\"\"\"\n",
    "\n",
    "answer = rag.query(query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "To answer your question, there are differences between majuscule (uppercase) and minuscule (lowercase) letters in the Latin alphabet.\n",
       "\n",
       "Majuscule refers to the uppercase form of a letter, which is used for writing large or formal documents, titles, and headings. Examples of majuscule letters include A, B, C, etc.\n",
       "\n",
       "Minuscule, on the other hand, refers to the lowercase form of a letter, which is used for everyday writing, such as in newspapers, books, and informal correspondence. Examples of minuscule letters include a, b, c, etc.\n",
       "\n",
       "In addition, the difference between majuscule and minuscule is also reflected in their visual appearance. Majuscule letters are typically larger and more distinct than minuscule letters.\n",
       "\n",
       "It's worth noting that the term \"majuscule\" was not commonly used until the 16th century, when the English language began to adopt a standardized alphabet based on the Latin alphabet. Prior to this time, the terms \"capital\" and \"minor\" were often used to refer to uppercase and lowercase letters, respectively.\n",
       "\n",
       "In summary, the main differences between majuscule and minuscule are:\n",
       "\n",
       "* The use of uppercase or lowercase for writing purposes\n",
       "* The visual appearance of the letters (majuscules are larger and more distinct)\n",
       "* The terminology used to describe these forms (majuscule refers to uppercase, while minuscule refers to lowercase)"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from IPython.display import Markdown, display\n",
    "\n",
    "display(Markdown(str(answer)))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ragEnv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
