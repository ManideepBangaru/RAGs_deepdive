{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dataset Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from glob import glob\n",
    "\n",
    "image_files = glob(\"data/social-posts/*.jpg\")\n",
    "\n",
    "text_files = glob(\"data/social-posts/*.txt\")\n",
    "\n",
    "print(len(image_files), len(text_files))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(image_files[1], text_files[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "documents = []\n",
    "\n",
    "for i in range(1, len(image_files)+1):\n",
    "    text_file  = f'data/social-posts/{i}.txt'\n",
    "    image_file = f'data/social-posts/{i}.jpg'\n",
    "    \n",
    "    text = open(text_file).read()\n",
    "    doc = {\"text\": text, \"image\":image_file}\n",
    "    documents.append(doc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "from PIL import Image\n",
    "\n",
    "num = random.randint(0, len(documents)-1)\n",
    "\n",
    "# print text\n",
    "print(documents[num][\"text\"])\n",
    "\n",
    "# display image\n",
    "display(Image.open(documents[num][\"image\"]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Embed Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install fastembed\n",
    "\n",
    "from fastembed import TextEmbedding, ImageEmbedding\n",
    "\n",
    "class EmbedData:\n",
    "    def __init__(self,\n",
    "                 documents,\n",
    "                 text_model_name=\"Qdrant/clip-ViT-B-32-text\",\n",
    "                 image_model_name=\"Qdrant/clip-ViT-B-32-vision\"):\n",
    "\n",
    "        # Initialize text embedding model\n",
    "        self.documents = documents\n",
    "        self.text_model = TextEmbedding(model_name=text_model_name)\n",
    "        self.text_embed_dim = self.text_model._get_model_description(text_model_name)[\"dim\"]\n",
    "        \n",
    "        # Initialize image embedding model\n",
    "        self.image_model = ImageEmbedding(model_name=image_model_name)\n",
    "        self.image_embed_dim = self.image_model._get_model_description(image_model_name)[\"dim\"]\n",
    "    \n",
    "    def embed_texts(self, texts):\n",
    "        text_embeddings = list(self.text_model.embed(texts))\n",
    "        return text_embeddings\n",
    "    \n",
    "    def embed_images(self, images):\n",
    "        image_embeddings = list(self.image_model.embed(images))\n",
    "        return image_embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddata = EmbedData(documents)\n",
    "\n",
    "embeddata.text_embeds  = embeddata.embed_texts([doc[\"text\"] for doc in documents])\n",
    "\n",
    "embeddata.image_embeds = embeddata.embed_images([doc[\"image\"] for doc in documents])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "print(np.array(embeddata.text_embeds))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Define vector store"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from qdrant_client import QdrantClient, models\n",
    "\n",
    "class QdrantVDB:\n",
    "    def __init__(self,\n",
    "                 collection_name,\n",
    "                 image_dim,\n",
    "                 text_dim,\n",
    "                 url=\"http://localhost:6333\"):\n",
    "\n",
    "        self.image_dim = image_dim\n",
    "        self.text_dim = text_dim\n",
    "        self.collection_name = collection_name\n",
    "        self.client = QdrantClient(url=url, prefer_grpc=True)\n",
    "    \n",
    "\n",
    "    def create_collection(self):\n",
    "\n",
    "        if not self.client.collection_exists(self.collection_name):\n",
    "        \n",
    "            print(f\"Creating collection '{self.collection_name}'...\")\n",
    "        \n",
    "            self.client.create_collection(\n",
    "                collection_name=self.collection_name,\n",
    "                \n",
    "                vectors_config={\n",
    "                    \"image\": models.VectorParams(size=self.image_dim,\n",
    "                                                 distance=models.Distance.COSINE),\n",
    "                    \"text\": models.VectorParams(size=self.text_dim,\n",
    "                                                distance=models.Distance.COSINE),\n",
    "                }\n",
    "            )\n",
    "        \n",
    "            print(f\"Collection '{self.collection_name}' created successfully.\")\n",
    "        \n",
    "        else:\n",
    "            print(f\"Collection '{self.collection_name}' already exists.\")\n",
    "        \n",
    "    def upload_embeddings(self, embeddata):\n",
    "\n",
    "        print(f\"Uploading points to collection '{self.collection_name}'...\")\n",
    "        \n",
    "        points = []\n",
    "        \n",
    "        for idx, doc in enumerate(embeddata.documents):\n",
    "            point = models.PointStruct(id=idx,  # Unique ID for each point\n",
    "                                       vector={\n",
    "                                           \"text\": embeddata.text_embeds[idx], \n",
    "                                           \"image\": embeddata.image_embeds[idx]\n",
    "                                           },\n",
    "                                       payload=doc  # Original image and its caption\n",
    "                                       )\n",
    "        \n",
    "            points.append(point)\n",
    "\n",
    "        self.client.upload_points(collection_name=self.collection_name, points=points)\n",
    "        \n",
    "        print(f\"Uploaded {len(points)} points to collection '{self.collection_name}'.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vector_db = QdrantVDB(\"linkedin-posts\",\n",
    "                      embeddata.image_embed_dim,\n",
    "                      embeddata.text_embed_dim)\n",
    "\n",
    "vector_db.create_collection()\n",
    "\n",
    "vector_db.upload_embeddings(embeddata)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Define Retriever class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Retriever:\n",
    "\n",
    "    def __init__(self, vector_db, embeddata):\n",
    "        \n",
    "        self.vector_db = vector_db\n",
    "        self.embeddata = embeddata\n",
    "\n",
    "    def search(self, query, limit=3):\n",
    "        query_embedding = list(self.embeddata.embed_texts(query))[0]\n",
    "\n",
    "        result = self.vector_db.client.search(\n",
    "                collection_name=self.vector_db.collection_name,\n",
    "                query_vector=(\"image\", query_embedding),\n",
    "                with_payload=[\"image\", \"text\"], \n",
    "                limit=limit\n",
    "            )\n",
    "\n",
    "        return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"What are some examples of Graph-based clustering algorithms?\"\n",
    "\n",
    "result = Retriever(vector_db, embeddata).search(query, limit=1)\n",
    "\n",
    "for i in result:\n",
    "    print(i.payload[\"text\"])\n",
    "    display(Image.open(i.payload[\"image\"]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RAG Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "import ollama\n",
    "\n",
    "class RAG:\n",
    "\n",
    "    def __init__(self,\n",
    "                 retriever,\n",
    "                 llm_name=\"llama3.2-vision\"):\n",
    "        \n",
    "        self.llm_name = llm_name\n",
    "        self.retriever = retriever\n",
    "        self.qa_prompt_tmpl_str = \"\"\"Context information is below.\n",
    "                                     ---------------------\n",
    "                                     {context}\n",
    "                                     ---------------------\n",
    "\n",
    "                                     Some images may also be available to you\n",
    "                                     for answering the question better. You have\n",
    "                                     to undersatnd those images thoroughly and \n",
    "                                     extra all relevant information that might \n",
    "                                     help you answer the query better.\n",
    "\n",
    "                                     ---------------------\n",
    "                                     \n",
    "                                     Given the context information above I want you\n",
    "                                     to think step by step to answer the query in a\n",
    "                                     crisp manner, incase case you don't know the\n",
    "                                     answer say 'I don't know!'\n",
    "                                     \n",
    "                                     ---------------------\n",
    "                                     \n",
    "                                     Query: {query}\n",
    "                                     \n",
    "                                     ---------------------\n",
    "                                     Answer: \"\"\"\n",
    "    \n",
    "    def generate_context(self, query):\n",
    "    \n",
    "        result = self.retriever.search(query)\n",
    "        context = [dict(data) for data in result]\n",
    "        combined_prompt = []\n",
    "\n",
    "        for entry in context:\n",
    "            context = entry[\"payload\"][\"text\"]\n",
    "\n",
    "            combined_prompt.append(context)\n",
    "\n",
    "        return \"\\n\\n---\\n\\n\".join(combined_prompt), result\n",
    "    \n",
    "    def query(self, query):\n",
    "        context, result = self.generate_context(query=query)\n",
    "        \n",
    "        prompt = self.qa_prompt_tmpl_str.format(context=context,\n",
    "                                                query=query)\n",
    "\n",
    "        messages = [\n",
    "                {\n",
    "                    \"role\": \"user\",\n",
    "                    \"content\": prompt,\n",
    "                    \"images\": [result[0].payload['image']]\n",
    "                },\n",
    "            ]\n",
    "        \n",
    "        response = ollama.chat(model=self.llm_name, messages=messages)\n",
    "    \n",
    "        return response"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Using RAG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "retriever = Retriever(vector_db, embeddata)\n",
    "\n",
    "rag = RAG(retriever)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"\"\"What are some examples of\n",
    "           Graph-based clustering algorithms?\"\"\"\n",
    "\n",
    "response = rag.query(query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"\"\"Are there any ways to\n",
    "           speed up native Python code?\"\"\"\n",
    "\n",
    "response = rag.query(query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"\"\"What is the mathematics behind the\n",
    "           kernel trick? Show me a step-by-step\n",
    "           explanation with the polynomial\n",
    "           kernel and two 2D vectors.\"\"\"\n",
    "\n",
    "response = rag.query(query)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ragEnv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
