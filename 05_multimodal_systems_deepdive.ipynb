{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/manideepbangaru/Documents/learnings/RAGs_deepdive/ragEnv/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "import random\n",
    "import torchvision.transforms as transforms\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from torchvision.datasets import MNIST\n",
    "from torch import optim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "mnist_train = MNIST(root='./data', train=True, download=True)\n",
    "mnist_test = MNIST(root='./data', train=False, download=True)\n",
    "\n",
    "transform = transforms.Compose([transforms.ToTensor()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SiameseDataset(Dataset):\n",
    "    def __init__(self, data, transform=None):\n",
    "        self.data = data\n",
    "        self.transform = transform\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.data)  # Add this method\n",
    "        \n",
    "\n",
    "    def __getitem__(self, index):\n",
    "    \n",
    "        imgA, labelA = self.data[index]\n",
    "            \n",
    "        same_class_flag = random.randint(0, 1) # pair with same class?\n",
    "        \n",
    "        if same_class_flag: # yes, pair with same class\n",
    "            labelB = -1\n",
    "            while labelB != labelA:\n",
    "                imgB, labelB = random.choice(self.data)\n",
    "                \n",
    "        else: # no, pair with different class\n",
    "            labelB = labelA\n",
    "            while labelB == labelA:\n",
    "                imgB, labelB = random.choice(self.data)\n",
    "\n",
    "        if self.transform:\n",
    "            imgA = self.transform(imgA)\n",
    "            imgB = self.transform(imgB)\n",
    "            \n",
    "        pair_label = torch.tensor([(labelA != labelB)], dtype=torch.float32)\n",
    "            \n",
    "        return imgA, imgB, pair_label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "siamese_train = SiameseDataset(mnist_train, transform)\n",
    "siamese_test = SiameseDataset(mnist_test, transform)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SiameseNetwork(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(SiameseNetwork, self).__init__()\n",
    "\n",
    "        self.cnn = nn.Sequential(\n",
    "            nn.Conv2d(1, 64, kernel_size=5, stride=1, padding=2),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.MaxPool2d(2, stride=2),\n",
    "\n",
    "            nn.Conv2d(64, 128, kernel_size=5, stride=1, padding=2),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.MaxPool2d(2, stride=2),\n",
    "\n",
    "            nn.Conv2d(128, 256, kernel_size=3, stride=1, padding=1),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.MaxPool2d(2, stride=2)\n",
    "        )\n",
    "\n",
    "        self.fc = nn.Sequential(\n",
    "            nn.Linear(256 * 3 * 3, 1024),\n",
    "            nn.ReLU(inplace=True),\n",
    "\n",
    "            nn.Linear(1024, 256),\n",
    "            nn.ReLU(inplace=True),\n",
    "\n",
    "            nn.Linear(256, 2)\n",
    "        )\n",
    "\n",
    "    def forward_once(self, x):\n",
    "        output = self.cnn(x)\n",
    "        output = output.view(output.size()[0], -1)\n",
    "        output = self.fc(output)\n",
    "        return output\n",
    "\n",
    "    def forward(self, inputA, inputB):\n",
    "        outputA = self.forward_once(inputA)\n",
    "        outputB = self.forward_once(inputB)\n",
    "        return outputA, outputB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ContrastiveLoss(torch.nn.Module):\n",
    "    \n",
    "    def __init__(self, margin=2.0):\n",
    "        super(ContrastiveLoss, self).__init__()\n",
    "        self.margin = margin\n",
    "\n",
    "    def forward(self, outputA, outputB, y):\n",
    "        euclidean_distance = F.pairwise_distance(outputA, outputB, keepdim = True)\n",
    "\n",
    "        same_class_loss = (1-y) * (euclidean_distance**2)\n",
    "        diff_class_loss = (y) * (torch.clamp(self.margin - euclidean_distance, min=0.0)**2)\n",
    "    \n",
    "        return torch.mean(same_class_loss + diff_class_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataloader = DataLoader(siamese_train, shuffle=True, num_workers=0, batch_size=64)\n",
    "model = SiameseNetwork()\n",
    "criterion = ContrastiveLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr = 0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0; Loss 286.3716132491827\n",
      "Epoch 1; Loss 105.9401145670563\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(5):\n",
    "    total_loss = 0\n",
    "    \n",
    "    for imgA, imgB, label in train_dataloader:\n",
    "\n",
    "        # imgA, imgB, label = imgA.cuda(), imgB.cuda(), label.cuda()\n",
    "        optimizer.zero_grad()\n",
    "        outputA, outputB = model(imgA, imgB)\n",
    "        loss_contrastive = criterion(outputA, outputB, label)\n",
    "        loss_contrastive.backward()\n",
    "\n",
    "        total_loss += loss_contrastive.item()\n",
    "        optimizer.step()\n",
    "\n",
    "    print(f\"Epoch {epoch}; Loss {total_loss}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CLIP (Contrastive Language-Image Pre-Training)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Training "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "data = load_dataset(\n",
    "    \"jamescalam/image-text-demo\",\n",
    "    split=\"train\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['text'][3]\n",
    "data['image'][3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import CLIPProcessor, CLIPModel\n",
    "import torch\n",
    "\n",
    "model_id = \"openai/clip-vit-base-patch32\"\n",
    "\n",
    "processor = CLIPProcessor.from_pretrained(model_id)\n",
    "model = CLIPModel.from_pretrained(model_id)\n",
    "\n",
    "# move model to device if possible\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = data['text']\n",
    "images = data['image']\n",
    "\n",
    "inputs = processor(text=text, \n",
    "                   images=images,\n",
    "                   return_tensors=\"pt\",\n",
    "                   padding=True\n",
    "                   ).to(device)\n",
    "\n",
    "print(inputs.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "outputs = model(**inputs)\n",
    "\n",
    "print(outputs.keys())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_emb = outputs.text_embeds\n",
    "image_emb = outputs.image_embeds\n",
    "\n",
    "print(text_emb.shape)\n",
    "print(image_emb.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_emb = text_emb / torch.norm(text_emb, dim=1, keepdim=True)\n",
    "\n",
    "image_emb = image_emb / torch.norm(image_emb, dim=1, keepdim=True)\n",
    "\n",
    "cos_sim = torch.mm(text_emb, image_emb.T).detach().numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.imshow(cos_sim)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Text to Image Retrieval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "query_text = \"Dog running on grass\"\n",
    "\n",
    "# preprocess text (tokenize, etc.)\n",
    "inputs = processor(text=[query_text], return_tensors=\"pt\", padding=True).to(device)\n",
    "\n",
    "# generate text embeddings\n",
    "text_features = model.get_text_features(**inputs)\n",
    "\n",
    "# normalize text embedding\n",
    "text_features = text_features / text_features.norm(dim=-1, keepdim=True)\n",
    "    \n",
    "# Calculate similarity scores across all image embddings\n",
    "similarity = torch.mm(text_features, image_emb.T)\n",
    "    \n",
    "# Get top-k matches\n",
    "top_k = 3\n",
    "values, indices = similarity[0].topk(min(top_k, len(data)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(1, top_k, figsize=(15, 3))\n",
    "    \n",
    "for i, (idx, score) in enumerate(zip(indices, values)):\n",
    "    # Print text and score\n",
    "    print(f\"{data['text'][idx]}: {score:.3f}\")\n",
    "        \n",
    "    # Display image\n",
    "    axes[i].imshow(data['image'][idx])\n",
    "    axes[i].axis('off')\n",
    "    axes[i].set_title(f\"Score: {score:.3f}\")\n",
    "    \n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Image to Image Retrieval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "query_image = data['image'][0]\n",
    "\n",
    "# preprocess image\n",
    "inputs = processor(images=query_image, return_tensors=\"pt\", padding=True).to(device)\n",
    "\n",
    "# generate image embeddings\n",
    "image_features = model.get_image_features(**inputs)\n",
    "\n",
    "# normalize image embedding\n",
    "image_features = image_features / image_features.norm(dim=-1, keepdim=True)\n",
    "    \n",
    "# Calculate similarity scores across all image embddings\n",
    "similarity = torch.mm(image_features, image_emb.T)\n",
    "    \n",
    "# Get top-k matches\n",
    "values, indices = similarity[0].topk(min(top_k, len(data)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(1, top_k, figsize=(15, 3))\n",
    "    \n",
    "for i, (idx, score) in enumerate(zip(indices, values)):\n",
    "    # Print text and score\n",
    "    print(f\"{data['text'][idx]}: {score:.3f}\")\n",
    "    \n",
    "    # Display image\n",
    "    axes[i].imshow(data['image'][idx])\n",
    "    axes[i].axis('off')\n",
    "    axes[i].set_title(f\"Score: {score:.3f}\")\n",
    "    \n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Zero-shot classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "import requests\n",
    "\n",
    "url = \"http://images.cocodataset.org/val2017/000000039769.jpg\"\n",
    "image = Image.open(requests.get(url, stream=True).raw)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = processor(\n",
    "    text=[\"a photo of a cat\", \"a photo of a dog\"],\n",
    "    images=image,\n",
    "    return_tensors=\"pt\",\n",
    "    padding=True,\n",
    ")\n",
    "\n",
    "outputs = model(**inputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logits_per_image = outputs.logits_per_image\n",
    "\n",
    "probs = logits_per_image.softmax(dim=1)\n",
    "\n",
    "print(probs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Multi-modal prompting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import ollama\n",
    "from IPython.display import Markdown, display\n",
    "\n",
    "response1 = ollama.chat(\n",
    "    model='llama3.2-vision',\n",
    "    messages = [{\"role\": \"user\",\n",
    "                \"content\": \"Who wrote the book Lord of the Rings?\"\n",
    "                }]\n",
    "                )\n",
    "\n",
    "display(Markdown(response1.message.content))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "response2 = ollama.chat(\n",
    "    model='llama3.2-vision',    \n",
    "    messages = [\n",
    "      {\"role\": \"user\",\n",
    "       \"content\": \"Who wrote the book Lord of the Rings?\"},\n",
    "       \n",
    "      {\"role\": \"assistant\",\n",
    "       \"content\": response1.message.content},\n",
    "      \n",
    "      {\"role\": \"user\",\n",
    "       \"content\": \"What other books has the author written?\"}\n",
    "    ],\n",
    ")\n",
    "\n",
    "display(Markdown(response2.message.content))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Conversation:\n",
    "    def __init__(self, system=\"\"):\n",
    "        self.messages = [] # define history list\n",
    "        \n",
    "        if system:\n",
    "            self.messages.append({\"role\": \"system\", \"content\": system})\n",
    "            \n",
    "    def generate(self, user_question):\n",
    "    \n",
    "        # append user query to history under \"user\" role\n",
    "        self.messages.append({\"role\": \"user\", \"content\":user_question})\n",
    "        \n",
    "        # generate response from LLM\n",
    "        response = ollama.chat(model='llama3.2-vision', messages=self.messages)\n",
    "        \n",
    "        # Add LLM's response to the history under \"assistant\" role\n",
    "        self.messages.append({\"role\":\"assistant\", \"content\":response.message.content})\n",
    "        \n",
    "        return response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define conversation\n",
    "system_message = \"You are a terse expert in high fantasy literature.\"\n",
    "conv = Conversation(system_message)\n",
    "\n",
    "# generate response from query\n",
    "response = conv.generate(\"Who wrote the book Lord of the Rings?\")\n",
    "\n",
    "# display response\n",
    "display(Markdown(response2.message.content))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(conv.messages)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Specify images in the prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import ollama\n",
    "\n",
    "response = ollama.chat(\n",
    "    model='llama3.2-vision',\n",
    "    messages=[{\n",
    "        'role': 'user',\n",
    "        'content': 'Describe this image.',\n",
    "        'images': ['dogs.png']\n",
    "    }]\n",
    ")\n",
    "\n",
    "display(Markdown(response.message.content))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import ollama\n",
    "\n",
    "response = ollama.chat(\n",
    "    model='llama3.2-vision',\n",
    "    messages=[{\n",
    "        'role': 'user',\n",
    "        'content': 'Describe this image.',\n",
    "        'images': ['dogs1.png', \"dogs2.png\"]\n",
    "    }]\n",
    ")\n",
    "\n",
    "display(Markdown(response.message.content))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## A realistic OCR use case"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import ollama\n",
    "\n",
    "image_path = \"image.png\"\n",
    "\n",
    "response = ollama.chat(\n",
    "                model='llama3.2-vision',\n",
    "                messages=[{'role': 'user',\n",
    "                           'content': \"\"\"Analyze the text in the provided image.\n",
    "                                         Extract all readable content and present\n",
    "                                         it in a structured Markdown format that\n",
    "                                         is clear, concise, and well-organized.\n",
    "                                         Ensure proper formatting (e.g., headings,\n",
    "                                         lists, or code blocks) as necessary to\n",
    "                                         represent the content effectively.\"\"\",\n",
    "                            'images': [image_path]\n",
    "                            }]\n",
    "                        )\n",
    "\n",
    "print(response.message.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tool Calling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Demo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import yfinance as yf\n",
    "import ollama"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_stock_price(ticker: str) -> float:\n",
    "    stock = yf.Ticker(ticker)\n",
    "    return stock.history(period='1d')['Close'].iloc[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "response = ollama.chat(\n",
    "    'llama3.2',\n",
    "    messages=[{'role': 'user', 'content': 'What is the stock price of Apple?'}],\n",
    "    tools=[get_stock_price],  # Pass the tool function reference\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(dict(response))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "available_functions = {\n",
    "    'get_stock_price': get_stock_price,\n",
    "}\n",
    "\n",
    "for tool in response.message.tool_calls or []:\n",
    "\n",
    "    function_to_call = available_functions.get(tool.function.name)\n",
    "\n",
    "    if function_to_call:\n",
    "        print('Arguments:', tool.function.arguments)\n",
    "        print('Function output:', function_to_call(**tool.function.arguments))\n",
    "\n",
    "    else:\n",
    "        print('Function not found:', tool.function.name)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ragEnv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
